{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data contest final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MRC0e0KhQ0S"
      },
      "source": [
        "# AdaBoost for dataset 2 and Logistic Regression for Dataset 1\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Lj-GdCwklJb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f37c0e4-804b-482d-fe50-02664bc5531c"
      },
      "source": [
        "#mounting the google drive to access the datasets\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWd1UlMnhT2s"
      },
      "source": [
        "## Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvGPUQaHhXfL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01bb66d4-3f60-4532-ebaf-7c42bf514359"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from imblearn.pipeline import Pipeline\n",
        "from imblearn.over_sampling import ADASYN\n",
        "from numpy import savetxt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzVfXvOzMkTQ"
      },
      "source": [
        "## Global Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcFQE4unMkwQ"
      },
      "source": [
        "path = '/content/drive/My Drive/PRML/Assignment2/Dataset/'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgY-_riNeb3q"
      },
      "source": [
        "#counts to check the data imbalance\n",
        "count1 = 0\n",
        "count2 = 0\n",
        "count3 = 0\n",
        "count4 = 0\n",
        "count5 = 0\n",
        "count6 = 0"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kGFpB_7a2lS"
      },
      "source": [
        "#setting the random seed\n",
        "np.random.seed(42)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1VMqkGvhc3-"
      },
      "source": [
        "## Importing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M52QDmyzhh9s"
      },
      "source": [
        "dataset_1 = pd.read_csv(path+'Dataset_1_Training.csv')\n",
        "X_train_1 = dataset_1.iloc[1:-2, 1:].values\n",
        "X_train_1 = X_train_1.T \n",
        "y_1_train = dataset_1.iloc[-2, 1:].values\n",
        "y_1_train = y_1_train.T\n",
        "y_1_train = y_1_train.astype('int') \n",
        "y_2_train = dataset_1.iloc[-1, 1:].values\n",
        "y_2_train = y_2_train.T\n",
        "y_2_train = y_2_train.astype('int') "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q32fqZOAbA6b"
      },
      "source": [
        "dataset_2 = pd.read_csv(path+'Dataset_2_Training.csv')\n",
        "X_train_2 = dataset_2.iloc[1:-4, 1:].values\n",
        "X_train_2 = X_train_2.T\n",
        "y_3_train = dataset_2.iloc[-4, 1:].values\n",
        "y_3_train = y_3_train.T\n",
        "y_3_train = y_3_train.astype('int') \n",
        "y_4_train = dataset_2.iloc[-3, 1:].values\n",
        "y_4_train = y_4_train.T\n",
        "y_4_train = y_4_train.astype('int') \n",
        "y_5_train = dataset_2.iloc[-2, 1:].values\n",
        "y_5_train = y_5_train.T\n",
        "y_5_train = y_5_train.astype('int') \n",
        "y_6_train = dataset_2.iloc[-1, 1:].values\n",
        "y_6_train = y_6_train.T\n",
        "y_6_train = y_6_train.astype('int') "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBWJK6NXjoPQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef642058-5084-4880-ac49-e5654a903c34"
      },
      "source": [
        "#Number of training samples available\n",
        "features1 = X_train_1.shape[0]\n",
        "features2 = X_train_2.shape[0]\n",
        "print (features1, features2)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "130 340\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dE8wrb_gmeh"
      },
      "source": [
        "#Count the number of samples which are 1s\n",
        "\n",
        "for i in y_1_train:\n",
        "  if i == 1:\n",
        "    count1 +=1\n",
        "\n",
        "for i in y_2_train:\n",
        "  if i == 1:\n",
        "    count2 +=1\n",
        "\n",
        "for i in y_3_train:\n",
        "  if i == 1:\n",
        "    count3 +=1\n",
        "    \n",
        "for i in y_4_train:\n",
        "  if i == 1:\n",
        "    count4 +=1\n",
        "    \n",
        "for i in y_5_train:\n",
        "  if i == 1:\n",
        "    count5 +=1\n",
        "    \n",
        "for i in y_6_train:\n",
        "  if i == 1:\n",
        "    count6 +=1"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ddq2DN1XiEVO",
        "outputId": "30f952b6-e21d-4446-8e6e-38e1dbb01351"
      },
      "source": [
        "print(count1,count2,count3,count4,count5,count6) #proof of data imbalance"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "33 53 83 51 146 200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ihekd05EM8QH"
      },
      "source": [
        "dataset_1_test = pd.read_csv(path+'Dataset_1_Testing.csv')\n",
        "X_test_1 = dataset_1_test.iloc[1:, 1:].values\n",
        "X_test_1 = X_test_1.T"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SC79XqBwciZf"
      },
      "source": [
        "dataset_2_test = pd.read_csv(path+'Dataset_2_Testing.csv')\n",
        "X_test_2 = dataset_2_test.iloc[1:, 1:].values\n",
        "X_test_2 = X_test_2.T"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kW3c7UYih0hT"
      },
      "source": [
        "## Feature Scaling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fQlDPKCh8sc"
      },
      "source": [
        "#USING STANDARD SCALAR FROM SKLEARN FOR STANDARDIZATION\n",
        "# sc1 = StandardScaler()\n",
        "# sc1 = sc1.fit(X_train_1)\n",
        "# X_train_1 = sc1.transform(X_train_1)\n",
        "# X_test_1 = sc1.transform(X_test_1)\n",
        "\n",
        "# sc2 = StandardScaler()\n",
        "# sc2 = sc2.fit(X_train_2)\n",
        "# X_train_2 = sc2.transform(X_train_2)\n",
        "# X_test_2 = sc2.transform(X_test_2)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjXkDo5Rco3H"
      },
      "source": [
        "#USING NORMALIZER FROM SKLEARN FOR NORMALIZATION\n",
        "# norm1 = Normalizer()\n",
        "# norm1 = norm1.fit(X_train_1)\n",
        "# X_train_1 = norm1.transform(X_train_1)\n",
        "# X_test_1 = norm1.transform(X_test_1)\n",
        "\n",
        "# norm2 = Normalizer()\n",
        "# norm2 = norm2.fit(X_train_2)\n",
        "# X_train_2 = norm2.transform(X_train_2)\n",
        "# X_test_2 = norm2.transform(X_test_2)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4K1cYiIDv1_"
      },
      "source": [
        "## Applying PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVWWpqYQDyRn"
      },
      "source": [
        "#APPLYING PCA FOR DIMENSIONALITY REDUCTION\n",
        "# from sklearn.decomposition import PCA\n",
        "# pca_1 = PCA(n_components = 100)\n",
        "# X_train_1 = pca_1.fit_transform(X_train_1)\n",
        "# X_test_1 = pca_1.transform(X_test_1)\n",
        "# X_test_2 = pca.transform(X_test_2)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HcsK16hEXnh"
      },
      "source": [
        "# pca_2 = PCA(n_components = 200)\n",
        "# X_train_2 = pca_2.fit_transform(X_train_2)\n",
        "# X_test_2 = pca_2.transform(X_test_2)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3Qe3GUGJfDU"
      },
      "source": [
        "# print(X_train_1.shape)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CE7JRItOutVj"
      },
      "source": [
        "## ADASYN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPTdawyLSfYi"
      },
      "source": [
        "Adaptive synthetic sampling is an oversampling technique used to handle unbalanced datasets. ADASYN() is the method from imblearn class which is used for this purpose. In this section, we have tried oversampling all 6 clinical descriptors according to its requirement using ADASYN()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFwPjWituwSk",
        "outputId": "4ec73010-cf6c-4397-9c80-d689b694c410"
      },
      "source": [
        "ada = ADASYN()#(random_state=0, ratio={'class1':count1, 'class2':features1 - count1})\n",
        "X_train_11, y_11_train = ada.fit_sample(X_train_1, y_1_train)\n",
        "X_train_12, y_22_train = ada.fit_sample(X_train_1, y_2_train)\n",
        "X_train_21, y_33_train = ada.fit_sample(X_train_2, y_3_train)\n",
        "X_train_22, y_44_train = ada.fit_sample(X_train_2, y_4_train)\n",
        "X_train_23, y_55_train = ada.fit_sample(X_train_2, y_5_train)\n",
        "X_train_24, y_66_train = ada.fit_sample(X_train_2, y_6_train)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb6jCOCQiAmP"
      },
      "source": [
        "## Training the models\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMMvp6DPAut0"
      },
      "source": [
        "#TRYING TO LEARN THE PARAMETERS FOR DATASET 1\n",
        "# lr_list = [0.005, 0.05, 0.01, 0.075, 0.1, 0.2, 0.25, 0.5, 0.75, 1]\n",
        "\n",
        "# for c in lr_list:\n",
        "#     classifier_1 = LogisticRegression(C = c)\n",
        "#     classifier_1.fit(X_train_1, y_1_train)\n",
        "#     print(\"Clinical Discriptor 1\")\n",
        "#     print(\"Learning rate: \", c)\n",
        "#     print(\"Accuracy score (training): {0:.3f}\".format(classifier_1.score(X_train_1, y_1_train)))\n",
        "\n",
        "# for c in lr_list:\n",
        "#     classifier_2 = LogisticRegression(C = c)#random_state=0, class_weight= 'balanced', max_iter=10000\n",
        "#     classifier_2.fit(X_train_1, y_2_train)\n",
        "#     print(\"Clinical Discriptor 2\")\n",
        "#     print(\"Learning rate: \", c)\n",
        "#     print(\"Accuracy score (training): {0:.3f}\".format(classifier_2.score(X_train_1, y_2_train)))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEhoValyTfzK"
      },
      "source": [
        "Logistic Regression gave the best performance among all the classifiers for dataset 1\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvtnEVJR-FzW",
        "outputId": "a42fd614-1da5-4923-d33f-51dd6ddbf42d"
      },
      "source": [
        "# classifier_1 = LogisticRegression(random_state=0)\n",
        "# classifier_1 = AdaBoostClassifier(n_estimators=8, learning_rate = 0.3)\n",
        "# classifier_1 = LogisticRegression(random_state=0, class_weight= 'balanced', max_iter=10000)\n",
        "\n",
        "classifier_1 = LogisticRegression()\n",
        "classifier_1.fit(X_train_11, y_11_train)\n",
        "\n",
        "# classifier_2 = LogisticRegression(random_state=0)\n",
        "# classifier_2 = AdaBoostClassifier(n_estimators=8, learning_rate = 0.3)\n",
        "# classifier_2 = LogisticRegression(random_state=0, class_weight= 'balanced', max_iter=10000)\n",
        "\n",
        "classifier_2 = LogisticRegression()\n",
        "classifier_2.fit(X_train_12, y_22_train)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ubfd6ZHB-cc"
      },
      "source": [
        "#TRYING TO LEARN THE PARAMETERS FOR DATASET 2\n",
        "# lr_list = [0.05, 0.075, 0.1, 0.2, 0.25,0.3,0.35,0.4,0.45, 0.5, 0.75, 1, 1.1]\n",
        "# for learning_rate in lr_list:\n",
        "#     classifier_3 = AdaBoostClassifier(n_estimators=20, learning_rate = learning_rate)\n",
        "#     classifier_3.fit(X_train_2, y_3_train)\n",
        "#     print(\"Clinical Discriptor 3\")\n",
        "#     print(\"Learning rate: \", learning_rate)\n",
        "#     print(\"Accuracy score (training): {0:.3f}\".format(classifier_3.score(X_train_2, y_3_train)))\n",
        "\n",
        "# for learning_rate in lr_list:\n",
        "#     classifier_4 = AdaBoostClassifier(n_estimators=20, learning_rate = learning_rate)\n",
        "#     classifier_4.fit(X_train_2, y_4_train)\n",
        "#     print(\"Clinical Discriptor 4\")\n",
        "#     print(\"Learning rate: \", learning_rate)\n",
        "#     print(\"Accuracy score (training): {0:.3f}\".format(classifier_4.score(X_train_2, y_4_train)))\n",
        "\n",
        "# for learning_rate in lr_list:\n",
        "#     classifier_5 = AdaBoostClassifier(n_estimators=20, learning_rate = learning_rate)\n",
        "#     classifier_5.fit(X_train_2, y_5_train)\n",
        "#     print(\"Clinical Discriptor 5\")\n",
        "#     print(\"Learning rate: \", learning_rate)\n",
        "#     print(\"Accuracy score (training): {0:.3f}\".format(classifier_5.score(X_train_2, y_5_train)))\n",
        "\n",
        "# for learning_rate in lr_list:\n",
        "#     classifier_6 = AdaBoostClassifier(n_estimators=20, learning_rate = learning_rate)\n",
        "#     classifier_6.fit(X_train_2, y_6_train)\n",
        "#     print(\"Clinical Discriptor 6\")\n",
        "#     print(\"Learning rate: \", learning_rate)\n",
        "    # print(\"Accuracy score (training): {0:.3f}\".format(classifier_6.score(X_train_2, y_6_train)))"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exk4p49PTkTC"
      },
      "source": [
        "Logistic Regression gave the best performance for the clinical descriptor 3,4 and Adaboost ensemble method gave the best performance for the clinical descriptor 5,6."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lp4PTrX--vUG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "077c6f71-f4fb-41a9-f432-ea9c1af1292b"
      },
      "source": [
        "classifier_3 = LogisticRegression()\n",
        "# classifier_3 = AdaBoostClassifier(n_estimators=20, learning_rate = 0.3)\n",
        "classifier_3.fit(X_train_21, y_33_train)\n",
        "\n",
        "classifier_4 = LogisticRegression()\n",
        "# classifier_4 = AdaBoostClassifier(n_estimators=20, learning_rate = 0.3)\n",
        "classifier_4.fit(X_train_22, y_44_train)\n",
        "\n",
        "# classifier_5 = LogisticRegression(random_state=0)\n",
        "classifier_5 = AdaBoostClassifier(n_estimators=20, learning_rate = 0.3)\n",
        "classifier_5.fit(X_train_23, y_55_train)\n",
        "\n",
        "# classifier_6 = LogisticRegression(random_state=0)\n",
        "classifier_6 = AdaBoostClassifier(n_estimators=20, learning_rate = 0.3)\n",
        "classifier_6.fit(X_train_24, y_66_train)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=0.3,\n",
              "                   n_estimators=20, random_state=None)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpmHL4d0N6Po"
      },
      "source": [
        "## Predicting the train set results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3v77B4SOT3L5"
      },
      "source": [
        "Predicting on training data to check the accuracy of our model on the train data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2u23NXLHOBkT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e72569df-7a60-4cef-8d51-e9e9dbc2fa37"
      },
      "source": [
        "print(X_train_1.shape, X_train_2.shape)\n",
        "y_pred_1_train = classifier_1.predict(X_train_11)\n",
        "y_pred_2_train = classifier_2.predict(X_train_12)\n",
        "y_pred_3_train = classifier_3.predict(X_train_21)\n",
        "y_pred_4_train = classifier_4.predict(X_train_22)\n",
        "y_pred_5_train = classifier_5.predict(X_train_23)\n",
        "y_pred_6_train = classifier_6.predict(X_train_24)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(130, 22282) (340, 54674)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKYVQH-l5NpE"
      },
      "source": [
        "## Predicting the Test set results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MK_D9X8RT_tZ"
      },
      "source": [
        "Predicting on test data to check the performance of our model on the test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6VMTb2O4hwM"
      },
      "source": [
        "y_pred_1 = classifier_1.predict(X_test_1)\n",
        "y_pred_2 = classifier_2.predict(X_test_1)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ds2C7_4fE4I"
      },
      "source": [
        "y_pred_3 = classifier_3.predict(X_test_2)\n",
        "y_pred_4 = classifier_4.predict(X_test_2)\n",
        "y_pred_5 = classifier_5.predict(X_test_2)\n",
        "y_pred_6 = classifier_6.predict(X_test_2)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BU8WWXZZg_ts"
      },
      "source": [
        "## Write into CSV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVN6ZmbIlPgy"
      },
      "source": [
        "#Concatenate the 6 predictions\n",
        "y_pred = np.concatenate((y_pred_1, y_pred_2, y_pred_3, y_pred_4, y_pred_5, y_pred_6), axis = 0)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OZ4tujUycqv"
      },
      "source": [
        "#makesure the predictions are of typr int\n",
        "y_pred = y_pred.astype('int') "
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xynMCKZdUUxB"
      },
      "source": [
        "Create a csv file in the format given"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEn8YxgmoENR"
      },
      "source": [
        "result = [ ]  \n",
        "k = 0\n",
        "for a in y_pred:\n",
        "  result.append([str(k),str(a)])\n",
        "  k = k + 1  \n",
        "result.insert(0, ['Id','Predicted'])"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Yg2pme816aN"
      },
      "source": [
        "savetxt('submission_AdaBoost5,6_LogReg1,2,3,4_estimators20_lr0-3_randomSeed42_ADASYN.csv', result,fmt = '%s', delimiter=',')#, header = \"Id, Prediction\")"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxQX-dHRIwZs"
      },
      "source": [
        "## Accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Git_ME0YUpMC"
      },
      "source": [
        "Check the accuracy of the predictions on the training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OthyMLpIzGM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b0477f0-379a-4f99-eb26-18511e8e98b9"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(y_11_train, y_pred_1_train)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MvczOu4OwVX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68669b05-999e-4d4b-e3e9-563f4fdd3577"
      },
      "source": [
        "accuracy_score(y_22_train, y_pred_2_train)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8ZlzW26Ox6W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c113aa9c-0177-4d37-e67f-96692cb11b25"
      },
      "source": [
        "accuracy_score(y_33_train, y_pred_3_train)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgf4cxf4O4DO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05331147-8325-42dc-d086-1d19e728d00e"
      },
      "source": [
        "accuracy_score(y_44_train, y_pred_4_train)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bk2UwJoO9wu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba53005c-ab3f-4c1f-e3f7-2363f2e06d27"
      },
      "source": [
        "accuracy_score(y_55_train, y_pred_5_train)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.991304347826087"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQa5uE-BO-cQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23fdc632-c0c0-4c5a-9025-d7cb37e2683a"
      },
      "source": [
        "accuracy_score(y_66_train, y_pred_6_train)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9074550128534704"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    }
  ]
}